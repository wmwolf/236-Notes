\documentclass[10pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Package Inclusion and Document Formatting %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage
{geometry,amsmath,amsthm,mathrsfs,amssymb,graphicx,bm,hyperref,url,pdfsync,
fancyhdr}
\pagestyle{fancy}
\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%
% Custom Commands %
%%%%%%%%%%%%%%%%%%%
\newcommand{\n}{\noindent}
\newcommand{\norm}[1]{\left\lvert#1\right\rvert}
\newcommand{\avg}[1]{\left\langle#1\right\rangle}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\figref}[1]{Figure \ref{#1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title Page Information %
%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Notes for PHYS 236: Cosmology}
\author{Bill Wolf}
\date{\today}

\begin{document}

\vfill\maketitle\vfill \newpage

\tableofcontents \newpage

%%%%%%%%%%%%%%%%%%%%%%
% January 9, 2013 %
%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction} % (fold)
\label{sec:introduction}
	\emph{Monday, January 9, 2013}\\
	
	\n Cosmology is essentially the study of the origin and evolution of the universe. The cosmologist attempts to discover if and how the universe is evolving, when it formed, how old/big it is, etc. Understanding the dynamics and geometry of the universe is essential for a complete picture of universal evolution. To date, we have a pretty convincing model, the Big Bang Theory (BBT), which explains a vast amount of observations, but questions remain. In addition, almost none of the predictions by the BBT can be measured in a laboratory, so we are forced to rely on observations.\\
	
	\n The basic idea of the BBT is that the universe began as a very hot place and has been expanding ever since. A period of extremely rapid expansion, called the inflationary period, caused small perturbations in the density of the universe to become macroscopic artifacts. The very much cooled and expanded universe is what we observe today, but in between, many interesting things must have happened, including the nucleosynthesis of primordial elements, ionization, and various changes to the rate of cosmic expansion.
	
	\subsection{The Long View} % (fold)
	\label{sub:the_long_view}
	
	Some classic examples of observations supported by the big bang theory include
	\begin{itemize}
		\item \textbf{Olbers' Paradox}: the universe, if infinite, should be infinitely bright, and yet we observe darkness
		\item \textbf{Hubble's Law}: far away galaxies are moving away from us at a rate proportional to their distance from us
		\item \textbf{The Cosmic microwave background (CMB)}: A nearly uniform background radiation at around 2.7 K as a perfect black body
		\item \textbf{He abundance in stars}: stars have nearly uniformly 28\% He when first born
	\end{itemize}
	However the BBT is not without flaws. Some difficulties remain, including
	\begin{itemize}
		\item \textbf{The Horizon Problem}: Points in the universe that are causally disconnected are very much similar. This is mostly solved by the theory of inflation.
		\item \textbf{The Flatness Problem}: The universe can only have three geometries: closed, open, or flat. There is no reason to expect the universe to be flat, and yet it appears to be flat to a very high precision. This, too, is solved by the inflation theory, which says that the universe will have expanded so greatly that any curvature is undetectable.
		\item \textbf{The Baryon Asymmetry Problem}: Why do we primarily have matter and very little anti-matter?
		\item \textbf{The Primordial Fluctuations Problem}: What caused the primordial fluctuations that gave rise to the first galaxies?
		\item \textbf{The Fine-Tuning Problem}: The energy densities of matter, radiation, curvature, and dark energy seem to be uniquely balanced to produce the flat universe that we observe. This is more of a philosophical problem than a scientific problem (think anthropic principle).
		\item \textbf{What is the Universe Made Of?} Two huge components of the universe are dark energy and dark matter. We know next to nothing about the nature of these mysterious forms of energy and cannot reproduce their effects in the laboratory.
	\end{itemize}
	There are also some interesting epistemological issues that arise from the fact that cosmology is not a classical scientific problem. We only have \emph{one} universe with \emph{one} specific location within that universe, and we can only observe from \emph{one} particular time in the evolution of this universe. We are thus very limited in our view of the universe, and we require assumptions to simplify our observations.\\
	
	\n One of those assumptions is the so-called \textbf{Cosmological Principle}, which postulates that our point of view is not special in any way. Earth is not in any particularly special point and we aren't in a particularly special time. This assumes that the laws of physics are independent of space and time. This is also a natural outgrowth of Ockham's razor (explanations should be as simple as possible). On large enough length scales, these assumptions are quite accurate, but we also see that on small enough scales, there are ``special'' places. For example, we are in a galaxy, but the majority of the universe is not galactic.
	% subsection the_long_view (end)	
	\subsection{The Game Plan} % (fold)
	\label{sub:the_game_plan}
		The course has a five-part plan:
		\begin{itemize}
			\item \textbf{Part 0}: Basic Phenomenology (L2-4)
			\item \textbf{Part I}: The (Smooth) Average Universe (L5-10)
			\item \textbf{Part II}: The Growth of Fluctuations (L11-L15)
			\item \textbf{Part III}: Very Early Universe (L20)
			\item \textbf{Part IV}: Class Presentations
		\end{itemize}
		To cover this material, we must introduce some particular units and concepts useful to cosmologists:
		\begin{itemize}
			\item \textbf{Parsec} (pc): The distance that gives a parallax of 1 arcsecond $\approx 3.08\times 10^{16}\,\mathrm{m}$
			\item \textbf{Solar Mass} ($M_\odot$): $\approx 2\times10^{30}\,\mathrm{kg} = 2\times 10^{33}\ \mathrm{g}$
			\item \textbf{Solar Luminosity} ($L_\odot$): $\approx 3.8\times 10^{26}\ \mathrm{W} = 3.8\times 10^{33}\ \mathrm{erg/s}$
			\item \textbf{Redshift} ($z$): $\equiv \lambda/\lambda_0-1$
			\item \textbf{Apparent Magnitude} ($m$): A measure of flux
			\begin{itemize}
				\item $m=-2.5\log F/F_0$ for some zero-point flux, $F_0$ (usually that of Vega)
				\item Letters indicate a particular band (filter) a magnitude is measured in (classic Johnson UBV filters, for example). There are \emph{many} different bands in use. Ex. $m_U$ or simply $U$ would be the $U$-band magnitude, or $m_U=-2.5 \log F_U/F_{0,U}$.
			\end{itemize}
			\item \textbf{Absolute Magnitude} ($M$) A measure of luminosity, defined as what the apparent magnitude of an object \emph{would} be if it were located at a distance of 10 pc.
			\begin{itemize}
				\item For the sun, $M_V = -4.83$ (the absolute magnitude in the $V$-band)
				\item For the Andromeda galaxy, $M_V\sim -20$
			\end{itemize}
		\end{itemize}
	% subsection the_game_plan (end)

% section introduction (end)	

\section{Phenomenology} % (fold)
\label{sec:phenomenology}
	One of the simplest indicators that our universe is not both infinite in extent and time (eternal and infinite) is \textbf{Olbers' Paradox}. This paradox states that if the universe were infinite and eternal, there should be a more or less uniform distribution of stars. Though the intensity of the stars' light would decrease by $1/r^2$, the number of emitters present also \emph{increases} by a factor of $r^2$, so the entire night sky should be as bright as the sun. Any light that runs into gas or something else would always be reprocessed, possibly at another wavelength, but it would still get to us. The BBT solves this paradox by limiting the \emph{age} of the universe. When looking further away, we are looking back in time, eventually reaching a point where stars did not exist yet, truncating the infinite star problem.\\
	
	\n While we can look back in time by looking further away, we unfortunately cannot look at the same place and see multiple times. Instead, we have to make theories about how parts of the universe at certain times evolve to other parts of the universe at later time.\\
	
	\n Another, more recent, observation that is indicative of the Big Bang is Hubble's law. Hubble looked at distant galaxies using Cepheid variables to determine their distance. He took spectra of these galaxies and noted that the spectral lines were nearly always shifted to the red. When he plotted this redshift against the distance, he found a linear relation between distance and velocity, namely
	\begin{equation}
		\label{eq:1} v = H_0 d
	\end{equation}
	where $v$ is the ``velocity'' (though the objects are really at rest\ldots more on this later), $H_0$ is the so-called \textbf{Hubble Constant} (which is not a real constant), and $d$ is the distance to the galaxy.\\
	
	\n A still more recent observation that supports the BBT is the observation of the \textbf{Cosmic Microwave Background}. When observing the universe in the microwave regime, and after subtracting out motion due to peculiar velocities, we see a nearly-perfect blackbody at $T=2.725\pm 0.001\ \mathrm{K}$. There are under-densities and over-densities that eventually give way to galaxies.\\
	
	\n Another interesting property of the universe that begs for understanding is the large-scale structure (LSS) of the universe. We see that galaxies are not uniformly distributed at random, at least on large scales. Rather, the universe is organized into filaments and walls of galaxies (so-called superclusters) and large voids that are under-dense in matter. This is a direct result of the action of heavy dark matter particles and the remnants of the fluctuations in the CMB.\\
	
	\n The basic building blocks in cosmology are galaxies rather than stars. A \textbf{galaxy} is a self-gravitating set of stars, gas, and dark matter. Their typical length scale is a kiloparsec for the baryonic matter and hundreds of kiloparsecs for the dark matter halo, a region with few baryons but relatively high dark matter density. In fact, most of the mass is contained in the dark matter halo, usually around $10^{7}$ to $10^{13}\,M_\odot$. The upper limit to the mass of galaxies is set by the dynamics of the collapse of structures, but the lower limit is a bit of a mystery that may have something to do with the physics of dark matter. The mass contained in stars is at most only a few percent, and the majority of the baryons are in the cold and hot gas known as the \textbf{interstellar medium}. Most galaxies are known to have many satellite and dark subhalos. \\
	
	\n Galaxies can be classified by their morphologies and are often organized by Hubble's tuning fork diagram with elliptical galaxies on the ``handle'' and normal and barred spiral galaxies on the ``tines''. The older elliptical galaxies typically showcase very little to no star formation and thus are red and ``dead'' (since blue stars are typically young). They are typically the largest galaxies with masses up to $10^{12}\,M_\odot$ and feature a hot, X-ray emitting halo. They are often found at the center of groups and clusters in highly clustered formations. With no clear axis of rotation, they are supported by pressure (rather than rotation) and their surface brightness can be described by De Vaucouleurs' law. There are also small elliptical galaxies, down to dwarf sizes that are typically younger and feature stronger rotational support and Seric brightness profiles.\\
	
	\n Spiral galaxies like the Milky Way and Andromeda, on the other hand, have younger stellar populations and thus appear bluer. They are rotationally supported and are rich in cold gas. The spiral ``arms'' are not really entities in and of themselves, but more so moving density waves. There are two main baryonic components to a spiral galaxy: its central bulge and the disk.\\
	
	\n A larger ``unit'' of cosmology is a galaxy cluster. \textbf{Galaxy Clusters} are massive systems, typically with $M> 10^{14}\,M_\odot$. They are often rich in very hot gas (with $kT\sim \mathrm{keV}$ and above), resulting in free-free emission in the X-ray. The gas in these clusters is largely believed to be pressure supported. These clusters are useful for many reasons, including their use as lenses for gravitational lensing and for probing the nature of the universe through their part in the Sunyaev-Zeldovich Effect, where high energy electrons in the cluster give energy to incoming CMB photons.
% section phenomenology (end)

% Monday, January 14, 2012

\section{Geometry in Cosmology} % (fold)
\label{sec:geometry_in_cosmology}
	\emph{Monday, January 14, 2012}\\

	\n We are lucky to be able to define a \textbf{cosmic time}, $t$ that all observers in the universe can supposedly agree on. We are able to define a \textbf{metric} that defines how distance is measured in the universe. In general, we can define many different metrics, but for a curved universe that is assumed to be homogeneous and isotropic, there are really only three options. There is the flat metric (Euclidean), the open metric (hyperbolic), and the closed metric (spherical). The non-flat geometries present some strange properties that we are not used to in everyday experiences. For instance, a circle about the North pole of a sphere will have a circumference smaller than $2\pi r$, where $r$ is the distance from the North pole to the circle, measured along the surface. Additionally, objects on the surface of a sphere appear to get first smaller as they travel away and then, once they pass the equator (when the observer is on a pole), they appear to get \emph{larger} again.\\
	
	\n In two dimensions, we can write the line element in Euclidean (flat) space in Cartesian or spherical coordinates as
	\begin{equation}
		\label{eq:geo:1}
		ds^2 = dx^2+ dy^2 = dr^2 + \Omega^2\,d\theta^2
	\end{equation}
	Though these metrics may appear different, they are really the same, just with a change of coordinates. In fact, a result of differential geometry is that no two physically different metrics can be made to look alike with coordinate changes. On the surface of a sphere, though, the metric must change. There is, after all, a maximum distance between two points now. 
	\begin{equation}
		\label{eq:geo:2} ds^2 = R_c^2\,d\theta^2 + R_c^2\sin^2\theta\,d\phi^2 = R_c^2\left(d\theta^2 + \sin^2\theta\,d\phi^2\right)
	\end{equation}
	Note that $\theta$ here is measured from the North pole, but the position of that pole is really arbitrary, so we are free to place it wherever we like, so long as we are consistent with the following calculations. Here $R_c$ is the radius of curvature. If there surface truly is the surface of a sphere in 3-space, this would be the radius of that sphere, though we need not require our two-dimensional space to be embedded in a three-dimensional space. An equivalent expression of the metric is
	\begin{equation}
		\label{eq:geo:3} ds^2 = dr^2 + R_c^2\sin^2\left(\frac{r}{R_c}\right)\,d\phi^2
	\end{equation}
	where $r=R_c\theta$. To be complete the corresponding metric for an negative (hyperbolic) curvature
	\begin{equation}
		\label{eq:geo:4} ds^2 = dr^2 + R_c^2\sinh^2\left(\frac{r}{R_c}\right)\,d\phi^2
	\end{equation}
	(No proof given because\ldots gross.)\\
	
	\n In three dimensions, the Euclidean metric generalizes in an obvious way, but for the spherical universe, we get
	\begin{equation}
		\label{eq:geo:5} ds^2 = dr^2 + R_c^2\sin^2\left(\frac{r}{R_c}\right)\left[d\theta^2 + \sin^2\theta\,d\phi^2\right]
	\end{equation}
	
	\n If we define
	\begin{equation}
		\label{eq:geo:6} x = \left\{\begin{matrix}
			R_c\sin(r/R_c) & \mathrm{Sphere}\\
			r & \mathrm{Flat}\\
			R_c\sinh(r/R_c) & \mathrm{Hyperbolic}
		\end{matrix}\right.
	\end{equation}
	Then the generalized metric is
	\begin{equation}
		\label{eq:geo:7} ds^2 = \frac{dx^2}{1-\kappa x^2} + x^2\left[d\theta^2+\sin^2\theta\,d\phi^2\right]
	\end{equation}
	where $\kappa=1$ for a closed (spherical) universe, $\kappa=0$ for a flat universe, or $\kappa=-1$ for an open universe. At this point, we have only considered static (time-independent) metrics. The obvious starting point for adding time-dependence is the Minkowski metric from special relativity:
	\begin{equation}
		\label{eq:geo:8} ds^2 = -dt^2 + dx^2 + dy^2 + dz^2
	\end{equation}
	The resulting, generalized metric is the \textbf{Robertson-Walker} metric:
	\begin{equation}
		\label{eq:geo:9} ds^2 = dt^2 - \frac{a^2(t)}{c^2}\left[dr^2 + R^2\sin^2\left(\frac{r}{R_c}\right)\left(d\theta^2+\sin^2\theta\,d\phi^2\right)\right]
	\end{equation}
	for closed space, or, more generally,
	\begin{equation}
		\label{eq:geo:10} ds^2 = dt^2-\frac{a^2(t)}{c^2}d\tilde{s}^2
	\end{equation}
	where $d\tilde{s}^2$ is the metric for the static geometry given in \eqref{eq:geo:7}. The function $a(t)$ controls the expansion or contraction of the manifold, and is called the \textbf{scale factor}. Understanding the evolution of $a(t)$ is crucial to our understanding of the history of the universe. Without knowing $a(t)$ a priori, we can still make a lot of statements about the universe's evolution for any generalized $a(t)$.\\
	
	\n As one concrete example, consider the motion of photons along the $r$-direction in flat space. Since photons travel along null geodesics ($ds^2=0$), we have
	\begin{equation}
		\label{eq:geo:11} 0=dt^2 - \frac{a^2(t)}{c^2}dr^2 \qquad \Rightarrow \qquad \frac{c\,dt}{a} = dr
	\end{equation}
	Suppose two successive wave crests of the photons were emitted at time $t_0$, separated by an interval $\Delta t_0$. They arrive to us at time $t_1$ separated by an interval $\Delta t_1$. Simply using \eqref{eq:geo:11}, we find
	\begin{equation}
		\label{eq:geo:12} \int_{t_1}^{t_0}\frac{c\,dt}{a(t)} = -\int_r^0dr =\int_{t_1+\Delta t_1}^{t_0+\Delta t_0}\frac{c\,dt}{a(t)} = \int_{t_1}^{t_0}\frac{c\,dt}{a(t)} + \frac{c\Delta t_0}{a(t_0)} - \frac{c\Delta t_1}{a(t_1)}
	\end{equation}
	Which tells us that
	\begin{equation}
		\label{eq:geo:13} 0 = \frac{c\Delta t_0}{a(t_0)} - \frac{c\Delta t_1}{a(t_1)} \qquad \Rightarrow \qquad \frac{\Delta t_0}{\Delta t_1} = \frac{a(t_0)}{a(t_1)} = \frac{1}{a(t_1)}
	\end{equation}
	where we've used the convention that $a(t_0)$ ($a$ now) is unity. The result here is that photons in an expanding universe \emph{lose energy}. This ratio is also related to the redshift, so we often define the redshift, $z$, via
	\begin{equation}
		\label{eq:geo:14} 1+z = \frac{1}{a(t)}
	\end{equation}
	This is actually an easily observed phenomenon for any thing sufficiently far away, though we do not regard these faraway objects as actually \emph{moving} per se. Rather, the space between objects is increasing, causing this observed redshift.
	
	\subsection{Hubble's Law} % (fold)
	\label{sub:hubble_s_law}
	Our earlier result tells us that
	\begin{equation}
		\label{eq:hubble:1} \int_{t_1}{t_0} \frac{c}{a(t)}dt = r
	\end{equation}
	If we consider small enough time intervals ($t_1\approx t_0$), the integrand is essentially a constant, and we get
	\begin{equation}
		\label{eq:hubble:2} r = \frac{c(t_0-t_1)}{a(t_0)}
	\end{equation}
	Now if we revisit our friend the redshift and do a taylor expansion around the present time, we get
	\begin{equation}
		\label{eq:hubble:3} z = \frac{a(t_0)}{a(t_1)}-1 = \frac{a(t_0)}{a(t_0)+(t_1-t_0)a'(t_0)}-1 = \frac{1}{1-(t_0-t_1)\frac{\dot{a}(t_0)}{a(t_0)}}-1\approx (t_0-t_1)\frac{\dot{a}(t_0)}{a(t_0)} = \frac{r}{c}\dot{a}(t_0)
	\end{equation}
	where in the last step we have used \eqref{eq:hubble:2}. So we have recovered the famous law of Hubble,
	\begin{equation}
		\label{eq:hubble:4} v\approx H_0 d
	\end{equation}
	where $H_0=\dot{a}(t_0)\approx 70\ \mathrm{km/s/Mpc}$. This required nothing other than general geometric arguments.\\
	
	\n A few notes on notation: often in the literature, $h$ will be used in place of $H_0$, where $h= H/100\ \mathrm{km/s/Mpc}$. Others still will use $h_{70} = H_0 / 70\ \mathrm{km/s/Mpc}$.
	% subsection hubble_s_law (end)
	\subsection{Cosmography} % (fold)
	\label{sub:cosmography}
		To measure distances in the universe, we employ the tools of Cosmography. There are many distances of interest. In fact, there are many differences to one particular location! We have the \textbf{angular diameter distance}, defined by
		\begin{equation}
			\label{eq:distances:1} D_A = \frac{d}{\theta}
		\end{equation}
		where $d$ is the known diameter of the object and $\theta$ is the angular diameter as measured from Earth. Another distance is the \textbf{luminosity distance}, defined by
		\begin{equation}
			\label{eq:distances:2} D_\mathrm{L}^2 = \frac{L}{4\pi F}
		\end{equation}
		This is used when comparing the observed flux from an object of known luminosity to get a distance. Note that these two distance are not, in general, equal. There is a factor of $(1+z)^2$ between the two. For a spherical geometry, where $R$ is the co-moving coordinate of an object (i.e., $r$ is invariant in time), these two distances are related via
		\begin{align}
			\label{eq:eq:distances:3} D_A &= \frac{R_c \sin(r/R_c)}{1+z}\\
			\label{eq:eq:distances:4} D_L &= D_A(1+z)^2
		\end{align}
		To move these results to flat or open geometry, the $\sin$ business would change accordingly. A detailed description of these discrepancies between the measurements of distance are given in Longair, but the short version of it is that in addition to the ``distance'' part that redshift gives to the distance, the redshift of the photons also robs the photons of their former count rate (by a factor of $1+z$) as well as their energy (by another factor of $1+z$).\\
	
		\n Another question one might ask is how much volume is contained between two shells at redshifts $z_1$ and $z_2$? This question is very important when making estimates of how many objects one might expect to see during survey (see exercises).\\
	
		\n The \textbf{lookback time} is an important quantity that tells us how far back in time we are looking at a particular object. Often we are interested in finding $t(z)$ to relate measured redshifts to lookback times. To do this, we need to use the \textbf{Hubble Parameter}:
		\begin{equation}
			\label{eq:distances:5} H(t) = \frac{\dot{a}(t)}{a(t)}
		\end{equation}
		Note that the Hubble constant is just the current value of the Hubble parameter. The calculation that needs to be done to find the lookback time is then
		\begin{equation}
			\label{eq:distances:6} t(z) = \int_0^z \frac{dz}{H(z)}\frac{1}{1+z}
		\end{equation}
	
		% Wednesday, January 16, 2013
		\textit{Wednesday, January 16, 2013}\\
	
		\n [MISSING STUFF]\\
	
		\n The lookback time can be expressed as a function of $z$ as
		\begin{equation}
			\label{eq:distances:7} t(z) = \int_0^z\frac{dz'}{H(z')(1+z')}
		\end{equation}
		We can also find the lookback distance as a function of $z$:
		\begin{equation}
			\label{eq:distances:8} r(z) = \int_0^z \frac{c\,dz'}{H(z')} = \int_{t_0}^\infty \frac{c\,dt}{a'(t)}
		\end{equation}
		If we extend $z\to\infty$ we find the age of the universe and the horizon.

	% subsection cosmography (end)
% section geometry_in_cosmology (end)
\section{Cosmic Dynamics} % (fold)
\label{sec:cosmic_dynamics}
	\subsection{Friedmann's Equation (Classical Derivation)} % (fold)
	\label{sub:friedmann_s_equation_classical_derivation_}
		Consider a sphere within a homogeneous and isotropic universe. We wish to find the equation of motion of the particles along the surface of this sphere. For convenience, consider the initial radius to be unity so that we can write the radius at any time as $a(t)$. The acceleration due to the gravitational pull would be
		\begin{equation}
			\label{eq:dynamics:1} \ddot{a}(t) = -\frac{GM}{a^2}=-\frac{4\pi}{3}G\rho a
		\end{equation}
		where in the second equality we have used the fact that density is constant. If we multiply both sides by $\dot{a}$, we get
		\begin{equation}
			\label{eq:dynamics:2} \ddot{a}(t)\dot{a}(t) = -\frac{4\pi}{3}G\rho a \dot{a}
		\end{equation}
		Integrating this, we find
		\begin{equation}
			\label{eq:dynamics:3} \frac{1}{2}\dot{a}^2=\frac{4\pi}{3}G\rho_0\frac{a_0^3}{a}+\frac{c}{2}
		\end{equation}
		If we define the \textbf{critical density} of the universe at the current time as
		\begin{equation}
			\label{eq:dynamics:4} \Omega_0=\frac{8\pi G \rho_0}{3H_0^2}
		\end{equation}
		and note that at the present time, the equation reads
		\begin{equation}
			\label{eq:dynamics:5} \frac{1}{2}\frac{\dot{a}^2(t_0)}{a_0^2} = \frac{4\pi}{3}G\rho_0\frac{a_0}{a_0} + \frac{c}{a_0^2}
		\end{equation}
		and finally defining $\dot{a}(t_0)/a(t_0)=H_0$, we may write
		\begin{equation}
			\label{eq:dynamics:6} 1 = \Omega_0 + \frac{c}{a_0^2H_0^2}
		\end{equation}
		This allows us to rewrite \eqref{eq:dynamics:3} as the ``fake'' \textbf{Friedmann's Equation}
		\begin{equation}
			\label{eq:dynamics:7} \left(\frac{\dot{a}}{a}\right)^2 = \Omega_0 H_0^2\left(\frac{a_0}{a}\right)^3+(1-\Omega_0)\left(\frac{a_0}{a}\right)^2
		\end{equation}
		This equation reflects three possibilities for the fate of the sphere. First, if the density is below $\Omega_0$, the sphere will expand forever and reach some asymptotic expansion velocity. If the density is exactly the critical density, the sphere will expand forever, but will asymptotically approach zero expansion velocity. Finally, if the sphere is denser than $\Omega_0$, the sphere will eventually collapse back on itself.\\
		
		\n To do this ``right'', we need the full machinery of Einstein's equation:
		\begin{equation}
			\label{eq:dynamics:8} R_{\mu\nu}-\frac{1}{2}g_{\mu\nu}R = \frac{8\pi G}{c^2}T_{\mu\nu}
		\end{equation}
		Where $R_{\mu\nu}$ is the Ricci Tensor, $R=R^\mu_{\phantom{\mu}\mu}$ is the Ricci scalar, and $T_{\mu\nu}$ is the stress-energy tensor of the universe, often taken to be that of a perfect fluid:
		\begin{equation}
			\label{eq:dynamics:9}
			T_{\mu\nu} = \begin{pmatrix}
			\rho & 0 & 0 & 0\\
			0 & p & 0 & 0\\
			0 & 0 & p & 0\\
			0 & 0 & 0 & p \end{pmatrix}
		\end{equation}
		where $P$ is the pressure and $\rho$ is the energy density. We won't go through the details here, but the corresponding version of Friedmann's equation as developed from general relativity is
		\begin{equation}
			\label{eq:dynamics:10} \left(\frac{\dot{a}}{a}\right)^2 = \frac{8\pi g\rho}{3}-\frac{\kappa c^2}{a^2 + \frac{1}{3}\Lambda}
		\end{equation}
		where $\kappa$ is the curvature ($-1$ for open, $0$ for flat, $+1$ for closed) term and $\Lambda$ is the cosmological constant (if it indeed exists).\\
		
		\n The second derivative equation gives
		\begin{equation}
			\label{eq:dynamics:11} \ddot{a} = -\frac{4\pi G}{3}a\left(\rho + \frac{3P}{c^2}\right)+\frac{1}{3}\Lambda a
		\end{equation}
		where $P$ is the pressure. Combining these results gives the ``better'' Friedmann's equation:
		\begin{equation}
			\label{eq:dynamics:12}\boxed{\left(\frac{\dot{a}}{a}\right)^2 = H_0^2\left[\Omega_0\left(1+z\right)^3+(1-\Omega_0-\Omega_\Lambda)(1+z)^2 + \Omega_\Lambda\right]}
		\end{equation}
		We've technically left out some terms with $\Omega_\gamma(1+z)^4$, which represents the energy density due to radiation, but in our current understanding, that term is negligible. Also, sometimes we see another $\Omega$ defined via $\Omega_\kappa = 1-\Omega_0-\Omega_\Lambda$ to be the energy density provided by curvature (whatever that means). If this is positive, we have open curvature. If it is zero, we have flat curvature, and if it is negative, we have closed curvature. Note that aside from the terms provided by $\Omega_\Lambda$ (and optionally $\Omega_\gamma$), this form of Friedmann's equation matches exactly with \eqref{eq:dynamics:7}.\\
		
		\n We should also consider the work done by an expanding universe. From thermodynamics, we have the classic $dU = -p\,dV$ relation, where for our purposes, $V=a^3$. Often we wish to express the pressure as some multiple of the energy density via an \textbf{equation of state}, namely
		\begin{equation}
			\label{eq:dynamics:13} \frac{p}{c^2} = w\rho
		\end{equation}
		where $w$ is the interesting parameter to be determined. Using this substitution in \eqref{eq:dynamics:11} gives
		\begin{equation}
			\label{eq:dynamics:14} \ddot{a} = -\frac{4\pi G}{3} a\rho(1+3w)+\frac{1}{3}\Lambda a
		\end{equation}
		Which then gives the differential relation
		\begin{equation}
			\label{eq:dynamics:15} d\rho\,a^3 c^2 + da\,3a^2\rho c^2 = -p 3a^2\,da
		\end{equation}
		and so the density (of any kind) follows
		\begin{equation}
			\label{eq:dynamics:16} \rho \propto (1+z)^{3+3w}
		\end{equation}
		We can write the full-blown form of Friedmann's equation, valid at all times as
		\begin{equation}
			\label{eq:dynamics:17} \boxed{H^2(z) = H_0^2\left[\Omega_\gamma(z+z)^4 + \Omega_0(1+z)^3 + \Omega_\kappa(1+z)^2 + \Omega_{\mathrm{de}}(1+z)^{3+3w}+\Omega_\Lambda\right]}
		\end{equation}
		This particular form allows for both a cosmological constant \emph{and} dark energy. Typically we just assume that $\Omega_{\mathrm{de}}$ is the cosomological constant (i.e., $w=-1$) and we drop the additional term. However, this form is strictly more general.
	% subsection friedmann_s_equation_classical_derivation_ (end)
	\subsection{Using Friedmann's Equation} % (fold)
	\label{sub:using_friedmann_s_equation}
		We can use Friedmann's equation to make many useful computations. For instance, the cosmic distance in a universe with only matter (all other $\Omega$'s vanish)
		\begin{equation}
			\label{eq:dynamics:18} r = \int_0^z \frac{c\,dz'}{H(z')} = \frac{2c}{H_0\Omega_0^2(1+z)}\left\{\Omega_0z+(\Omega_0-1)\left[(\Omega_0z+1)^2-1\right]\right\}
		\end{equation}
		If we expand this to first order in $z$, we recover Hubble's Law: $D\approx zc/H_0$. Now if we assume $\Omega_0=1$, which is the Einstein-de Sitter case, this result simplifies to
		\begin{equation}
			\label{eq:dynamics:19} D = \frac{2c}{H_0}\left[2-\frac{1}{\sqrt{1+z}}\right]
		\end{equation}
		\subsubsection{The Einstein-de Sitter Case} % (fold)
		\label{ssub:the_einstein_de_sitter_case}
			In the Einstein-de Sitter universe, Friedmann's equation simplifies dramatically to
			\begin{equation}
				\label{eq:dynamics:20} H^2(z) = H_0^2\Omega_0(1+z)^3
			\end{equation}
			or simply
			\begin{equation}
				\label{eq:dynamics:21} H(z) = H_0\sqrt{\Omega_0}(1+z)^{3/2}
			\end{equation}
			Choosing $a$ over $z$ and $H$ gives us
			\begin{equation}
				\label{eq:dynamics:22} \frac{\dot{a}}{a} = H_0\Omega_0^{1/2}\frac{1}{a^{3/2}}
			\end{equation}
			We can solve for $a(t)$ (which essentially tells us everything) relatively simply:
			\begin{equation}
				\label{eq:dynamics:23} \frac{da}{a}a^{3/2} = a^{1/2}\,da = H_0\Omega_0^{1/2}\,dt
			\end{equation}
			The solution for $a(t)$ is
			\begin{equation}
				\label{eq:dynamics:24} a = \left[\frac{3}{2}H_0 \Omega_0^{1/2}(t-t_0)\right]^{2/3}+1
			\end{equation}
			Solving for the current age of the universe in this universe gives us
			\begin{equation}
				\label{eq:dynamics:25} t_0 = \frac{2}{3}H_0^{-1}
			\end{equation}
			This actually causes problems since we know of objects that are older than this, so we know that we do not live in an Einstein-de Sitter universe.\\
		% subsubsection the_einstein_de_sitter_case (end)
	% subsection using_friedmann_s_equation (end)
% section cosmic_dynamics (end)
\section{Cosmography} % (fold)
\label{sec:cosmography}
\n \textit{Wednesday, January 23, 2013}\\

	\n Much of observational cosmology focuses on finding a few particular numbers. $H_0$, $q_0$ (the deceleration parameter), $t_0$ (the age of the universe), $\Omega_i$ (the density parameters) all are crucial to our understanding of the evolution of the universe, so measuring them accurately is of the utmost importance. 
	\subsection{The Search for the Hubble Constant} % (fold)
	\label{sub:the_search_for_h_0_}
	A decent part of the last century in cosmology has been invested in getting an accurate measurement of the Hubble constant, $H_0$. Originally it was prioritized so that we could determine distance to objects at small redshift with relative accuracy, but more recently we've found other reasons to want to know the value of $H_0$ to within one percent accuracy.\\
	
	\n Now, if we have an accurate measurement of $H_0$, we can constrain values of other cosmological parameters, like $w$, the dark energy equation of state parameter. In the past, these cosmological parameters were dealt with on an individual basis, but modern experiments treat the problem in a multi-dimensional fashion, giving simultaneous ranges for multiple cosmological parameters.\\
	
	\n Over time, the value of $H_0$ has fallen from over $h=6$ (from Hubble's first measurements) in the 1920's down to between $h=0.5$ to $h=1.0$ from about 1960-2000. The varied values during those forty years was likely due to under-estimated error bars, which caused quite a kerfuffel among cosmologists. In the last decade, the measurements are converging towards $h=0.7$ to within about ten percent. A new generation of experiments hopes to get the value to within one percent.
	
	\subsubsection{The Distance Ladder} % (fold)
	\label{ssub:the_distance_ladder}
		Unfortunately, when measuring redshifts of distant galaxies, it is very difficult to separate the effects from cosmic expansion from the doppler shift due to peculiar velocities. To calibrate Hubble's law properly, we need to determine distances through some other means. In determining distances at various length scales around the universe, astronomers make use of the so-called \textbf{distance ladder}, which is a series of methods of determining the distance to an object, with each method reaching further than the last, but leaning on the accuracy of the previous method.\\
		
		\n The ``base'' method is the familiar parallax method for determining the distance to nearby stars. For slightly further objects (the Magellanic clouds, nearby galaxies), we can use Cepheid variable stars to determine a distance from the period luminosity relation (Cepheids pulsate with a frequency that is directly related to their luminosity). Since these can be used as a standard candle, we can ascertain a distance from a flux measurement and the $1/r^2$ law. With a second method of distance determination, we can find what the Hubble flow really is, allowing us to compensate for peculiar velocities. The Key Project aimed to do this and came up with a measurement of $H_0 = 72 \pm 7\ \mathrm{km/s/Mpc}$.
	% subsubsection the_distance_ladder (end)
	\subsubsection{Beyond the Key Project} % (fold)
	\label{ssub:beyond_the_key_project}
		Rather than using Cepheids, newer studies are using type Ia supernovae, bypassing a calibration via the LMC. The idea is that the uncertainty from LMC measurements will be avoided, resulting in a more precise measurement. This process, along with some other breakthroughs allowed Riess et al.~(2011) to reduce the uncertainty down to three percent in the SHOES project.\\
		
		\n Competing work from the Carnegie Hubble Project (CHP) instead looked at the mid-infrared for Cepheids which reduced uncertainty in other areas and came to about the same value of $H_0$ with the same uncertainty.
	% subsubsection beyond_the_key_project (end)
	% subsection the_search_for_h_0_ (end)
	\subsection{More Distant Measurements} % (fold)
	\label{sub:more_distant_measurements}
		To probe for effects apparent at higher redshifts, we need a robust standard candle. Many options were tried, like the brightest galaxy in a cluster or other less successful methods. To date, the best tool is are type Ia supernovae, which are thought to be the thermonuclear detonation of a C-O white dwarf. The actual progenitor of this system is not known, but they seem to have a uniform luminosity at peak luminosity. Actually, the luminosity is not quite standard, but it is calibrated by the width (in time) of the light curve (often referred to as the ``stretch''). In the future, light curves in the IR will be used since they appear to be truly standardized (``stretch''-independent). There are some difficulties to overcome in this. Errors due to interstellar reddening and atmospheric extinction make high-precision photometry very difficult.\\
		
		\n Recent work using Ias as standard candles seem to favor a cosmological constant for dark energy and $\Omega_m\sim 0.27$. However, several challenges still stand in the realm of Type Ia supernova cosmology, including the poorly understood physics behind their creation, selection effects (the brightest events are most likely to be detected, biasing our results), unknown dust contamination, and precise photometric calibration, as mentioned earlier.
	% subsection more_distant_measurements (end)
	\subsection{Cosmic Chronometers} % (fold)
	\label{sub:cosmic_chronometers}
	Aside from computing distances and the Hubble constant, we also can glean some information about the universe if we can find its age. We mentioned earlier that for an Einstein-de Sitter universe, the age is $t_0 = \frac{2}{3}H_0^{-1}$. If we found the age of the universe to be this, it would be one piece of evidence for (or against) a particular cosmological model. Stellar evolution puts a lower limit on the age of the universe since we see globular clusters with stars around 12 Gyrs old. To date, really all we can do is put lower limits on the age of the universe. It's very difficult to estimate the age of very distant objects, especially since galaxies change over time. As such, this is still an emerging subfield in cosmology. More methods are needed to corroborate the measurements made to date.
	% subsection cosmic_chronometers (end)
% section cosmography (end)
\section{Gravitational Lensing} % (fold)
\label{sec:gravitational_lensing}
	% Wednesday, January 30, 2013
	\textit{Wednesday, January 30, 2013}\\
	\subsection{Strong Lensing} % (fold)
	\label{sub:strong_lensing}
	\textbf{Strong Lensing} refers to the effect of extremely massive objects distorting the incoming light from objects behind. Often the ``lenses'' are large clusters of galaxies changing the image of galaxies behind them. The object behind is the source, and we speak of it being located in the \textbf{source plane}. The lensing object then deflects light from the source through a deflection angle. This can cause the perceived angular diameter of the object to change. Essentially the lens acts as a highly nonlinear mapping from $\mathbb{R}^2$ to $\mathbb{R}^2$. We will often refer to the distance from the deflector to the observer as $D_{\mathrm{d}}$, the distance from the deflector to the source as $D_{\mathrm{ds}}$, and the distance from the source to the observer as $D_{\mathrm{s}}$.\\
	
	\n The lensing causes a change in optical path length, but this also affects the arrival time of light from the source. The effect can be expressed mathematically via
	\begin{equation}
		\label{eq:strong_lensing:1} t(\vec{\theta}) = \frac{1+z_d}{c}\frac{D_{\mathrm{d}}D_{\mathrm{s}}}{D_{\mathrm{ds}}}\left[\frac{1}{2}(\vec{\theta}-\vec{\beta})^2-\psi(\vec{\theta})\right]
	\end{equation}
	Where $\beta$ is the angle between the source and the deflector with respect to the source plane and $\theta$ is the observed angular diameter of the lensed source. The factor of $(\vec{\theta}-\vec{\beta})^2$ is due to the geometry and is the geometric time delay, but the $\psi(\vec{\theta})$ is the gravitational effect, essentially accounting for the fact that the photons must travel along longer geodesics in deeper gravitational wells. There are many useful equations that can be derived from this phenomenon that allows us to learn about cosmic distances, but I do not list them here. They are well documented in the lecture slides and the reference paper.\\
	
	\n There are many reasons to study strong lensing. When observing a lensing event, we can very accurately measure the mass enclosed within the Einstein radius, the projected orientation of the mass, the projected ellipticity of the mass, and sometimes the derivative of the enclosed mass. The lensing matter need not be visible, so this is great for probing dark matter distributions. The strengths of strong lensing as a method are that the mass is independent of a dynamic state and that we can measure the total mass (regardless of the nature of that mass). Weaknesses lie in the susceptibility to projection effects and that we can \emph{only} measure total mass.\\
	
	\n Most relevant for our purposes is using strong lensing to do cosmography with arrival time delays. Since the time delay is proportional to the Fermat distance, $D_{\mathrm{d}}D_{\mathrm{s}}/D_{\mathrm{ds}}$, we gain information about the Hubble constant and some function of the cosmological parameters,
	\begin{equation}
		\label{eq:strong_lensing:2} \Delta t \propto D_{\mathrm{\Delta t}}(z_s,z_d)\propto H_0^{-1} f(\Omega_m,w,\ldots)
	\end{equation}
	After measuring the time delay and solving for the gravitational potential, in theory this can translate into the desired cosmological parameters.

	% subsection strong_lensing (end)

% section gravitational_lensing (end)
\section{Thermal History of the Universe} % (fold)
\label{sec:thermal_history_of_the_universe}
For the nonrelativistic case ($kT\gg mc^2$), a Maxwell-Boltzmann distribution of energies gives a number density 
\begin{equation}
	\label{eq:thermal:1} n = \frac{g}{h^3}\int_0^\infty f(\mathbf{p})\,d^3p = g\left(\frac{2\pi mkT}{h^2}\right)^{\frac{3}{2}} e^{-\frac{mc^2-\mu}{kT}}
\end{equation}
In the relativistic case, however, where $kT\gg mc^2$ and $kT\gg\mu$, things become simpler:
\begin{equation}
	\label{eq:thermal:2} n_b = \frac{g\zeta(3)}{\pi^2} \left(\frac{2\pi k T}{hc}\right)^3
\end{equation}
and
\begin{equation}
	\label{eq:thermal:3} n_f = \frac{3}{4}\frac{g\zeta(3)}{\pi^2} \left(\frac{2\pi k T}{hc}\right)^3
\end{equation}
for bosons and fermions, respectively. For the current CMB with $T\approx 2.7\ \mathrm{K}$, this corresponds to a number density of around $4\times 10^8\ \mathrm{m^{-3}}$. Compare that to the baryonic number density, which is about $1\ \mathrm{m^{-3}}$, so there are about a billion photons per baryon in the universe. The corresponding amount of energy is roughly equivalent to what would be gained by fusing all hydrogen in the universe into helium.\\

\n Since the radiation energy density scales as $(1+z)^4$ and matter energy density scales only as $(1+z)^3$, there was a time when radiation dominated the universe. In fact, aside from a few awkward transitions, the universe has always been dominated by a single component. We happen to live in the awkward transition from matter to dark energy right now. As such, many calculations for the evolution of the scale factor become trivial when in the regime of a single-component universe.

\subsection{Era of Recombination} % (fold)
\label{sub:era_of_recombination}
	There came a time when the CMB became cool enough to allow electrons to bind to ions and form neutral atoms. For some reason, this is called \textbf{Recombination}. We would expect this to occur when $kT_{\mathrm{CMB}}\sim 13.6\,\mathrm{eV}$. To do this correctly, we need to use the Saha equation, which gives us the number densities of species at various stages of ionization:
	\begin{equation}
		\label{eq:recomb:1} n_i = g_i \left(\frac{2\pi m_i kT}{h^2}\right)^{3/2}e^{-\frac{\mu -m_i c^2}{kT}}
	\end{equation}
	whilst requiring conservation of chemical potential:
	\begin{equation}
		\label{eq:recomb:2} \mu(e^-) + \mu(p^+) = \mu(\mathrm{H})
	\end{equation}
	Manipulating the three corresponding version of \eqref{eq:recomb:1} and using \eqref{eq:recomb:2} gives us
	\begin{equation}
		\label{eq:recomb:3} \frac{n_{\mathrm{H}}}{n_{e}n_p} = \frac{g_\mathrm{H}}{g_eg_p} \left(\frac{h^2}{2\pi m_ekT}\right)^{3/2} e^{-\frac{13.6\,\mathrm{eV}}{kT}}
	\end{equation}
	If we define $x=n_p/n_{\mathrm{\rm B}} = \frac{n_e}{n_{\rm B}}$ where ``B'' stands for baryons (protons or hydrogen atoms), we can re-express \eqref{eq:recomb:3} in terms of this ionization fraction:
	\begin{equation}
		\label{eq:recomb:4} \frac{1-x}{x^2n_{\rm B}} = \left(\frac{h^2}{2\pi m_e kT}\right)^{3/2}e^{-\frac{13.6\ \mathrm{eV}}{kT}}
	\end{equation}
	now if $\eta$ is the ratio of photon number density and baryon number density, we can rewrite this as
	\begin{equation}
		\label{eq:recomb:5} \frac{1-x}{x^2} = \eta n_\gamma\left(\frac{h^2}{2\pi m_e kT}\right)^{3/2} e^{-\frac{-13.6\,\mathrm{eV}}{kT}} = \eta\frac{\zeta(3)}{\pi^2}\left(\frac{2\pi k T}{hc}\right)^3 \left( \frac{h^2}{2\pi m_e kT}\right)^{3/2} e^{-\frac{13.6\,\mathrm{eV}}{kT}}
	\end{equation}
	where in the last form, we used \eqref{eq:thermal:2}. This is the Saha equation, albeit simplified since we neglect the effects of helium and other interfering influences in the universe. When $x$ is large, there are many free electrons, which cause Thomson scattering to affect the paths of photons. Because of this, we are unable to see beyond this ``surface of last scattering'' where the universe itself is effectively opaque. This occurred at about $z\sim 1000$, when matter still dominated the universe. We can do some ugly mathematics to calculate the optical depth due to this Thomson scattering as a function of redshift:
	\begin{equation}
		\label{eq:recomb:6} \tau_T = 0.035\frac{\Omega_b}{\Omega_m^{1/2}}hz^{3/2}\qquad [z\gg 1;\ x=1]
	\end{equation}
	\subsection{Radiation Dominated Era} % (fold)
	\label{sub:radiation_dominated_era}
	At first, baryons and radiation were coupled through Thomason scattering, so photons could not travel freely about the universe. After recombination, they decouple, and each component expanded adiabatically with temperature scaling via $T\sim 1/a$ for photons and $T\sim 1/a^2$ for baryons. The exchange of energy from photons to electrons via scattering gives us a temperature change rate of
	\begin{equation}
		\label{eq:radiation:1} \frac{dT_e}{dt} = \frac{4}{3}\sigma_T\epsilon_r\left(\frac{T_r-T_e}{m_ec}\right)
	\end{equation}
	And since the heat capacity of the CMB is much higher, changes in the electron temperature occur on a timescale much shorter than the age of the universe:
	\begin{equation}
		\label{eq:radiation:2} \tau = \frac{T_e}{dT_e/dt} = 7.4\times 10^{19}z^{-4}\ \mathrm{s}
	\end{equation}
	A similar process occurs for the hydrogen atoms later in time, but since the number of free electrons is decreasing, this effect is depressed, and we get effective decoupling by $z\sim 150$. After this time, photons and matter essentially stop communicating with each other.\\
	% subsection radiation_dominated_era (end)
% subsection era_of_recombination (end)
% section thermal_history_of_the_universe (end)

\n MISSING NUCLEOSYNTHESIS MATERIAL (KIND OF IMPORTANT)
\section{Growth of Structure} % (fold)
\label{sec:growth_of_structure}
	% Wednesday, February 13, 2013
	\textit{Wednesday, February 13, 2013}\\
	
	\n In the study of structure formation, we wish to understand one important quantity, $\Delta = \delta\rho/\rho$, where $\rho$ is the average background density of the universe. This gives us a sense of how strong a density perturbation is and whether or not it will result in the creation of structure. For small $\Delta$, we are in the linear regime, and the analytics are relatively simple. When $\Delta\sim 1$, things become more complex. The density perturbations typically grow large enough to be ground by gravity and detach from the Hubble Flow. Are interest presently is to investigate this regime of density formation. Current densities of galaxies and clusters show they must have collapsed at $z\ll 100$, well after recombination. Before then, everything was linear.\\
	
	\n Jeans instability is a slow process in an expanding universe, as opposed to the rather violent event known from stellar formation and other applications. Expanding the fluid equations about the average density of the universe gives us
	\begin{equation}
		\label{eq:struc1} \frac{d^2\Delta}{dt^2} + 2H \frac{d\Delta}{dt} = \frac{c_s^2}{\rho_0 a^2}\nabla_c^2\delta\rho + 4\pi G \delta \rho
	\end{equation}
	the presence of the Hubble parameter separates this from normal Jeans theory and that found in an expanding universe. We can recast this in Fourier space (now with $H=0$ to find growing and decaying modes, given by the dispersion relation
	\begin{equation}
		\label{eq:struc2} \omega^2 = k^2c_s^2 - 4\pi G\rho
	\end{equation}
	We also get a characteristic Jeans' length:
	\begin{equation}
		\label{eq:struc3} \lambda_j  c_s\left(\frac{\pi}{G\rho}\right)^{1/2}
	\end{equation}
	For density perturbations that are larger than the Jeans' length, collapse ensures. Smaller perturbations are supported by pressure.\\
	
	\n However, this process is \emph{damped} by the expansion of the universe (the $H$ term).  The fluid equation in Fourier space is 
	\begin{equation}
		\label{eq:struc4} \frac{d^2\Delta}{dt^2}+2H\frac{d\Delta}{dt} = \Delta(4\pi G\rho_0-k^2c_s^2)
	\end{equation}
	For an Einstein-de Sitter universe, the results is that $\Delta\propto t^{2/3}$ so that $\Delta\propto a\propto (1+z)^{-1}$. That is, instabilities only grow linearly. This is far too slow to create the large-scale structure that we see now from the small instabilities observed in the CMB. In fact, more realistic cosmologies cause even more damping. For a long time, this was seen as a huge problem in our understanding of cosmology and galactic formation.\\
	
	\n Note that this analysis only holds for small perturbations since they are Newtonian in nature. Soon we will see the more general picture that comes from GR. Additionally, there is no ``gravitational crossing time'' for communication in this picture. That is, gravity acts \emph{instantaneously} in this current analysis, rather than taking a light crossing time. However, even in full-blown GR theory, this January molasses-like structure formation is still observed. It turns out that dark matter has to save the day yet again to produce the relatively rapid formation of galaxies and clusters.\\
	
	\n An important point here is that observing the growth of structure provides an additional constraint on the Hubble parameter, assuming we understand the growth of structure well enough to probe this damping term.\\
	
	\n As density perturbations grow, velocity dispersions also grow according to
	\begin{equation}
		\label{eq:struc5} \frac{d\mathbf{u}}{dt} + 2H\mathbf{u} = -\frac{1}{a^2}\nabla_c\delta\phi
	\end{equation}
	since as voids become voider and over-densities become more overdense, matter is attracted more and more away from the voids. This spawned another industry in determining the Hubble constant via studying peculiar velocities at various redshifts. Measuring the peculiar velocities of clusters depends strongly on finding the distances to said clusters, which is not a simple task. Because of this difficulty, this approach is not taken much anymore to constrain cosmological parameters.\\
	
	% Wednesday, February 20, 2013
	\n\textit{Wednesday, February 20, 2013}\\
	
	\n We've looked at small-scale perturbations, but it's important to consider perturbations on the scale of the horizon. This requires a full-blown GR treatment. The punchline is that these perturbations grow with $a^2$ in the radiation-dominated universe and with $a$ in the matter-dominated universe. At some point, though, the perturbation become encompassed within the horizon and more interesting physics takes over.\\
	
	\n We've been throwing the word ``horizon'' around willy nilly, but we should be more precise. The \textbf{particle horizon} is the maximum [SLIDE ADVANCED TOO QUICKLY] and the \textbf{event horizon} is [SOMETHING ELSE]. The particle horizon is what we must compare the size of perturbations. Then in addition to the particle horizon length, the relevant length scale is
	\begin{equation}
		\label{eq:struc6} \lambda_{j,\mathrm{non-rel}} = c_s\left(\frac{\pi}{G\rho}\right)^{1/2} \qquad \lambda_{j,\mathrm{rel}}=c_s\left(\frac{3\pi}{8G\rho}\right)^{1/2}
	\end{equation}
	which are the non-relativistic and relativistic Jeans' lengths, respectively. We must also consider the diffusion lengths, which are important when the fluid approximation we have been using implicitly breaks down. This is important both for photons (in the case of silk damping), and for collisionless particles (the free streaming length, which we will discuss later).\\
	
	\n Consider now a universe made of only two components: baryons and photons. Under an adiabatic perturbation, both photon and baryon density (and thus pressure) are perturbed. In the radiation-dominated phase, the horizon is approximately the jeans length, so any perturbation is stabilized when it enters the horizon. After recombination, though, the Jeans length drops, allowing collapse, but photon diffusion damps the perturbation below the threshold mass (the silk mass).\\
	
	\n The other type of perturbations are isothermal perturbations, where only baryons are perturbed against a constant temperature background photon gas. This case is similar to the adiabatic case, but there is no photon diffusion.\\
	
	\n The speed of sound is an important quantity for stability considerations, and it is given by
	\begin{equation}
		\label{eq:struc7} c_s = \frac{(\partial p/\partial T)_S}{(\partial \rho_r/\partial T)_S+(\partial \rho_m/\partial T)_S} = \frac{c^2}{3}\frac{4\rho_r}{4\rho_r+3\rho_m}
	\end{equation}
	Thus at early, radiation-dominated times, $c_s\approx c/\sqrt{3}$. At later times, though, the sound speed drops dramatically, especially since after recombination, baryons only feel their own pressure, which is \emph{much} smaller than that of the photons. In the radiation dominated era, then, the Jeans length is
	\begin{equation}
		\label{eq:struc8} \lambda_J = c\left(\frac{3\pi}{24G\rho}\right)^{1/2}
	\end{equation}
	which is very near the horizon length scale. Thus, up until close to recombination, overdensities above the silk mass will not grow or decay but sit there. Very close to recombination, the jeans length diverges from the horizon length, so very large structures are allowed to collapse, but after recombination, the jeans mass drops appreciably, so the ``frozen-in'' overdensities collapse into structures, though the overdensities that were smaller than the silk mass at recombination were washed away by silk damping (this process ceases after recombination, where photons are decoupled from the baryons).\\
	
	\n Up to recombination, the silk mass was given by
	\begin{equation}
		\label{eq:struc9} M_S = 1.3\times 10^{12}\left(\Omega_0h^2\right)^{-3/2}\ M_\odot
	\end{equation}
	This means that silk damping effectively destroys all fluctuations below this mass scale, which is a huge problem since it doesn't allow for any structure formation on galactic scales before recombination. This gives rise to an interesting universe where the \emph{first} things to collapse are those with masses above $\sim 10^{14}\ M_\odot$, known lovingly as Zeldovich pancakes, since they would inevitably flatten along one axis. We know now that this model doesn't work, thus the early universe cannot have been a baryon-photon fluid with adiabatic perturbations.\\
	
	\n Perhaps the perturbations where isothermal, so that the low mass overdensities would survive recombination. This is known as the \textbf{baryonic-isothermal model}. The primordial perturbations are set up early on, but silk damping does not destroy them. Then after recombination, these smaller structures are allowed to collapse. The jeans mass is of order $10^5-10^6\ M_\odot$, which would indicate that these first structures could be the globular clusters that we know are the oldest structures in the universe. This is essentially what we believe today and is the basis for the idea of \textbf{hierarchical clustering}, where smaller structures combine to make progressively larger structures.\\
	
	\n Unfortunately, this understanding does not allow enough time for structure formation since recombination. A much stronger set of CMB fluctuations must be present to account for the relatively rapid formation of large-scale structure. We will find that dark matter solves this problem by speeding up structure formation. Other problems with this baryon-photon fluid model are that big bang nucleosynthesis shows that $\Omega_b \ll 1$. We also know that $\Omega_m> \Omega_b$ from many other sources. Additionally, we will see that the CMB requires two ``fluids'' (rather than just the one baryon-photon fluid).
	
	
% section growth_of_structure (end)

\section{Dark Matter} % (fold)
\label{sec:dark_matter}
	% Wednesday, February 27, 2013
	\textit{Wednesday, February 27, 2013}\\
	
	\n [MISSED FIRST THIRTY MINUTES. I SUCK AT WAKING UP.]\\
	
	\n Dark matter is weakly interacting (i.e.\ it only interacts gravitationally and possibly via the weak force). Thus, dark matter particles have a very long mean free path. However, the fluid approximation is only valid for scales much larger than the mean free path. Thus, the free streaming length sets a minimum scale for perturbation in collisionless dark matter [INSERT LOTS OF MATH HERE]:
	\begin{equation}
		\label{eq:dm:1} \frac{\ell_{\mathrm{FS}}}{a} = \left\{\begin{array}{l l}
			\frac{2ct}{a}\propto a & \\
			\frac{2ct_{\mathrm{NR}}}{a_{\mathrm{NR}}}\left[1+\ln\left(\frac{a}{a_{\mathrm{NR}}}\right)\right] & \\
			\frac{2ct_{\mathrm{NR}}}{a_{\mathrm{NR}}}\left[\frac{5}{2}+\ln\left(\frac{a_{\mathrm{eq}}}{a_{\mathrm{NR}}}\right)\right] & t \gg t_{\mathrm{eq}}
		\end{array}\right.
	\end{equation}
	To date, there have been a couple reported detections of dark matter particles, but these are not taken seriously. The detection threshold of both WIMP mass and baryon-WIMP cross section is getting ever lower as a series of experiments are slated to come online in the coming years.\\
	
	\n In addition to structure formation and velocity curves, we have further evidence for dark matter in interacting galaxy clusters like the bullet cluster. When two clusters collide, the hot intergalactic gas from each cluster interacts and produces X-Rays, but the dark matter halo presses on, carrying stars with it. So we can see a distinct X-ray emitting region surrounded by over-densities in stars.\\
	
	\n The LHC is also attempting to put limits on dark matter self-interaction cross-sections. In 2012, Dawson et al.\ reported a marginal detection that is again not taken very seriously. Fermi is attempting to put limits on dark matter self-annihilation and there has supposedly been a detection at 130 GeV, but again, you guessed it, this isn't taken very seriously.\\
	
	\n We studied the growth of structure in a universe absent dark matter. But since dark matter is collisionless and interactions with the photon-baryon gas are negligible, subhorizon scale perturbations are not damped as they are for the photon-baryon gas.

% section dark_matter (end)

\section{Large Scale Structure} % (fold)
\label{sec:large_scale_structure}
	For a long time, we have known that galaxies are not randomly distributed in the universe, but instead appear in clusters. The theory we have thus far described is a linear theory, but galaxies are complex, collapsed objects that do not follow the simple theory. Additionally, galaxies are not the same as dark matter, further complicating our study.\\
	
	\n We need an understanding of the power spectrum of primordial fluctuations. That is, we'd like to see if we can take the current observed distribution of clumps in the universe and map it back to the primordial fluctuations. There are two ``simple'' steps to take us from primordial fluctuations to the large scale structure we observe today.\\
	
	\subsection{Galaxy Clustering} % (fold)
	\label{sub:galaxy_clustering}
		The probability of finding two galaxies next to each other is larger than random. That is, there is a nonzero correlation function that tells us how likely there is to be a galaxy within some unit angle $\theta$ from a given galaxy. The number galaxies found in some volume $dV$ is given by
		\begin{equation}
			\label{eq:large:1} dN(r) = N_0\left[1+\xi(r)\right]\,dV
		\end{equation}
		where
		\begin{equation}
			\label{eq:large:2} \xi = \left(\frac{r}{r_0}\right)^{-\gamma}
		\end{equation}
		where $r_0$ is the \textbf{correlation length}. For our measurements, we find
		\begin{equation}
			\label{eq:large:3} \gamma\sim 1.8 \qquad r_0\sim 8\ \mathrm{Mpc}
		\end{equation}
		This can be mapped into an angular correlation function:
		\begin{equation}
			\label{eq:large:4} w(\theta) \sim \theta^{-\gamma+1}
		\end{equation}
		This power law approach works decently, but not perfectly. A common trend here is that the further away from a galaxy we look, the less effect the galaxy cluster has on the likelihood of finding another galaxy at that given distance (or angle). In addition, we see the correlation becomes stronger for brighter galaxies. That is, the presence of a bright galaxy is indicative of a higher probability of finding another nearby galaxy.\\
		
		The correlation function is, unsurprisingly, connected to density perturbations in the early universe. What we can do (see the slides for the equations) is measure $\xi$ and invert to find the distribution of primordial density perturbation. There are, however, two big problems. How do we connect the power spectrum now, $\Delta_k$ to the primordial one? Additionally, how do we connect the galaxy (cluster) power spectrum to the underlying mass power spectrum? In the linear (small perturbation) limit, we may assume
		\begin{equation}
			\label{eq:large:5} \Delta_k(z=0) = T(k) f(z) \Delta_k(z)
		\end{equation}
		where $T(k)$ is the \textbf{transfer function} that deals with the physics of damping and amplification, while $f(z)$ deals with the redshift-dependent effects (largely gravity). In the linear regime, we also relate the galactic distribution and dark matter distribution via a linear approximation:
		\begin{equation}
			\label{eq:large:6} \xi_{\mathrm{gal}} = b^2\xi_{\mathrm{DM}};\quad P_{\mathrm{gal}} = b^2P_{\mathrm{DM}};\quad \Delta_{\mathrm{gal}} = b\Delta_{\mathrm{DM}}
		\end{equation}
		where $b$ is the \textbf{bias} provided by dark matter clustering effects. These two problems are essentially what people are currently studying in the field of large scale structure.
	% subsection galaxy_clustering (end)
	\subsection{Initial Power Spectrum} % (fold)
	\label{sub:initial_power_spectrum}
		For a power law form (which is approximately is what is observed), we assume
		\begin{equation}
			\label{eq:large:7} P(k) = \norm{\Delta_k}^2 = A k^n
		\end{equation}
		which yields
		\begin{equation}
			\label{eq:large:8} \xi\propto r^{-n-3}
		\end{equation}
		And so
		\begin{equation}
			\label{eq:large:9} \Delta(M) = \avg{\Delta^2}^{1/2} \propto M^{-(n+3)/6}
		\end{equation}
		which is essentially the average amplitude of fluctuations as a function of mass. This is a very simple and tractable model that does not really model the observable universe well. A better model is the \textbf{Harrison-Zeldovich Spectrum}, which assumes $n=1$. In this prescription, early fluctuations outside the horizon grow as $a^2$, but the horizon also grows as $t\sim a^2$ during the radiation-dominated era.  Then the matter fluctuations have amplitude
		\begin{equation}
			\label{eq:large:10} \Delta(M) \propto a^2 M^{-(n+3)/6}
		\end{equation}
		where the horizon mass scales as
		\begin{equation}
			\label{eq:large:11} M_H\propto a^{-3}
		\end{equation}
		This model, in addition to be simple, gives very good predictions.\\
		
		\n Regarding the transfer function, during the radiation dominated era, dark matter perturbations on the subhorizon scale grew slowly since they were ``held back by stable radiation''. Then we would expect the transfer function to drop for scales smaller than horizon at the time of equality ($k>k_{\mathrm{eq}}$), where
		\begin{equation}
			\label{eq:large:12} k_{\mathrm{eq}}\approx 7.3\times 10^{-2}\Omega_0 h^2\ \mathrm{Mpc}^{-1}
		\end{equation}
		where $k_{\mathrm{eq}}$ is the wavenumber corresponding to the wavelength equal to the horizon scale at the time of matter-radiation equality. In hot dark matter, free streaming kills power on small scales faster.
	% subsection initial_power_spectrum (end)
	\subsection{Measuring Fluctuations} % (fold)
	\label{sub:measuring_fluctuations}
	We measure fluctuations through various techniques. At increasingly large lengthscales, we use: intergalactic hydrogen clumping (Lyman alpha forest), gravitational lensing, cluster abundance, SDSS galaxy clustering, and CMB measurements. Additionally, each of these methods probes a particular redshift range suitable to the physics of interest. So far, this all seems consistent with a $\Lambda$CDM universe.\\
	
	\n We can also measure the bias ($\delta_m/\delta_{\mathrm{DM}}=b$) by modeling the dark matter fluctuations from the CMB and comparing that to the observed galactic overdensities.\\
	
	\n A current favorite method of measuring these fluctuations is that of Baryonic Acoustic Oscillations (BAO). This relies on the fact that a perturbation actually causes a sound wave. However, this sound wave lasts temporally until the photons decouple from the baryons. Consider the case of a matching perturbation in baryons, photons, and dark matter in the early (radiation-dominated) universe. The photons will act to send waves out from the initial perturbation in both the baryons and the photons. However, the dark matter doesn't care since it doesn't interact with photons (and the baryonic mass is too weak to affect it). Thus, the dark matter continues to collapse. Then, once decoupling between photons and baryons occurs, the baryon waves begin to collapse again on the still intact dark matter perturbation (though they would collapse again without the dark matter). This then sets off an oscillation of expanding and contracting overdensities, whose frequencies tell us something about the composition and amplitude of the perturbation.\\
	
	\n BAOs are seen with little uncertainty in the data now and give independent measurements of cosmological parameters. This is a relatively new technique since accurate BAO signatures require extremely large volumes of survey data. BAO provides a lengthscale that is frozen at the moment of recombination, which serves as a standard ruler, which is how we get the cosmological parameters (much like the standard candle of the type Ia supernovae).\\


	% subsection measuring_fluctuations (end)

% section large_scale_structure (end)
\section{The Cosmic Microwave Background} % (fold)
\label{sec:the_cosmic_microwave_background}
\subsection{The Last Scattering Surface} % (fold)
\label{sub:the_last_scattering_surface}
	\textit{Wednesday, March 6, 2013}\\
	
	\n For a star, the ``last scattering surface'' is the photosphere. That is, the photons we see now last interacted with matter at the photosphere. For the CMB, though, the last scattering surface is a sphere \emph{around us}. Everywhere we look, we will eventually see this last scattering surface. However, the CMB last scattering surface, like a photosphere, does have a finite thickness:
	\begin{equation}
		\label{eq:cmb:1} v(z)\,dz = e^{-\tau}\frac{d\tau}{dz}dz
	\end{equation}
	where $\tau$ is the optical depth and $v$ is the \textbf{visibility}. This visibility varies with cosmic history as ionization occurs and reoccurs, causing spikes in thomson scattering's effects on the optical depth. We find a thickness of the last scattering surface of $\Delta z\sim 200$ located at a redshift of $z\sim 1000-1200$, which corresponds to a comoving thickness of $ \sim 40\ \mathrm{Mpc}$, or 3 arcminutes. The Silk damping scale at recombination was $\sim \sqrt{ct/3}\sim 9\ \mathrm{Mpc}$, whereas the sound horizon was approximately $c_s t\sim 56\ \mathrm{Mpc}$.
% subsection the_last_scattering_surface (end)
	
\subsection{The CMB Power Spectrum} % (fold)
\label{sub:the_cmb_power_spectrum}
	To get a power spectrum, we expand $\delta T/T$ in spherical harmonics. For symmetry, the coefficients of spherical harmonics are related to the power on an angular scale $\theta \approx \pi/l$. That is, once we know the power on some multipole moment $l$, we can translate that into a rough angular scale. Many plots will show both the multipole moment and the corresponding angular scale.\\
	
	\n On scales larger than the particle horizon ($\sim 2\ \mathrm{deg}$), the power spectrum will be close to the initial perturbations. In addition, photons will have to climb out or down from initial large scale perturbations (Sachs-Wolf effect) and then through perturbations along the line of sight (Integrated Sachs-Wolf (ISW) and Rees-Sciama). These alterations give us information about the growth of structure since the era of recombination.\\
	
	\n At intermediate lengthscales, we observe peaks in the power spectrum, called \textbf{acoustic peaks}. These are sensitive primarily to the time from the big bang to recombination, but also to the sound speed. As such, we can get information about the ratio of baryons to dark matter by looking at the ratios of these peaks. Essentially we have that the location of the first peak tells you the total mass content of the universe, whereas the ratios of the peak heights give you the ratio of baryonic matter to dark matter.\\
	
	\n On small scales ($l>2000$), Silk damping becomes important. Additionally, statistical damping on scales smaller than the thickness of \ldots\\
	
	\n Finally, the reionization of the universe at $z\sim 10$ effects the observed CMB power spectrum. The new electrons add an additional optical depth, which attenuates the overall amplitudes of the fluctuations. However, the locations of the fluctuations are not affected, so the CMB can give us information about the optical depth to the last scattering surface.
% subsection the_cmb_power_spectrum (end)
\subsection{Neutrinos and the CMB} % (fold)
\label{sub:neutrinos_and_the_cmb}
	The CMB can also tell us information about the primordial abundances of various energy forms. Extra relativistic species will act to change the expansion speed and hence, the sound horizon. This, in tern affects the locations of the peaks in the CMB power spectrum. This introduces degeneracies between the cosmological parameters, the number of neutrinos, and primordial elemental abundances. This makes the CMB a little bit lss of the Swiss army knife that we've made it out to be. However, setting the distance to the last scattering surface can limit how many neutrino species there are. At present, we are confident that there are no more than three neutrino species, so this doesn't appear to be too terrible of a problem.\\
	
	\n Additionally, we have put an upper limit on the sum of the masses of all neutrino species. Combined with BAO and other parameters, we know that $\sum m_\nu < 0.44\ \mathrm{eV}$. 
% subsection big_bang_nucleosynthesis_and_the_cmb (end)
% section the_cosmic_microwave_background (end)
\end{document}

